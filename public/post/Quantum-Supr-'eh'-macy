---
title: "Putting the 'Eh' in Quantum Supremacy"
date: 2019-10-01
draft: true
---

A lot of pixels have been spilled about the implications of Google's 'quantum supremacy' paper, which was accidentally released (leaked?) a few weeks ago. While we're still waiting to see the final version, there's not much reason to expect it will differ substantially from the leaked copy.

In a way, the importance of the result is a barometer for your feelings about quantum computing in general. Some extremely <a href="https://www.scottaaronson.com/blog/?p=4317#comment-1819916">bearish skeptics</a> don't think the result will stand. Other folks in the business are impressed, ecstatic and very optimistic _[a few links to tweets or blog posts?]_. The cryptocurrency fanatics are, of course, worried someone can steal their monopoly money in a new, much fancier and more expensive way, rather than with plain ol' monetary fraud _[a few links from cryptosites]_.

Others have offered explanations _[Starts with a bang?]_ and FAQs _[S. Aaronson]_ of and about the quantum supremacy experiment. I don't have much reason to doubt the result, yet. Google's experimental team is definitely world class and likely haven't committed rookie errors like failing to validate results from intermediate experiments (they published the result of the same algorithm on <a href="https://arxiv.org/pdf/1709.06678.pdf">9 qubits</a>).

Yet, I find it hard to be particularly excited about this. Aside from Scott Aaronson's forthcoming paper on that uses the random circuit protocol to generate certified random bits, this whole endeavor is basically useless. Indeed, the quantum supremacy paper itself ends with the line "we are now one clever algorithm away from a new era of computing" _[check this quote]_, admitting that no one knows what to do with this.

Maybe this will silence some QC skeptics, but very few people believe QC to be _impossible_ rather than just extraordinarily difficult. Is this pointless milestone going to be the last such accomplishment for 20 years? There are two major challenges here- scale and error correction. Google's previous attempt at scale, the 72-qubit 'Bristlecone' chip, suffered from too high error rates and thus was not suitable for this experiment. No fault tolerant logical qubit exists. Even worse, we basically have no idea what it would even look like. Similarly, individual microwave control of qubits is simply incompatible with a scaling to even hundreds of qubits, much less the thousands-millions of qubits required for a universal quantum computer.

Options:
- useful algorithm for 'NISQ' computer
- major progress toward fault tolerant error corrected qubits
- major progress on control scalability


_[check 9-qubit fidelities and such to compare with 'Sycamore']_
